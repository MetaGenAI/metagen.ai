---
layout: project
page: transflower
title: Transflower
description: Probabilistic autoregressive dance generation with multimodal attention
authors:
    - name: Author1
      url: aa
    - name: Author2
      url: bb
affiliations:
    - name: Affiliation1
---

<div class="centered assets">
    <ul>
        <li>
            <div class="project-button">
            <a href="https://github.com/MetaGenAI/MetaGenNeos" target="_blank">
                <p><i class='fa fa-file-alt'></i></p>
                <p>Paper</p>
            </a>
            </div>
        </li>
        <li>
            <div class="project-button">
            <a href="https://github.com/MetaGenAI/MetaGenNeos" target="_blank">
                <p><i class='fa fa-database'></i></p>
                <p>Dataset</p>
            </a>
            </div>
        </li>
        <li>
            <div class="project-button">
            <a href="https://github.com/MetaGenAI/MetaGenNeos" target="_blank">
                <p><i class='fa fa-github'></i></p>
                <p>Code</p>
            </a>
            </div>
        </li>
    </ul>
</div>

...

# Overview

<!-- image here? -->

Video here

Dance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.

# Generated dance samples

# Architecture

<div id="transflower-visualization"></div>

## Pretrained models

# Dance dataset

---

### BibTex


### Acknowledgements