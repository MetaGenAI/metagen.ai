---
page: mocagen
---

## Dance

<iframe width="560" height="315" src="https://www.youtube.com/embed/uBnCePehA-Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</br>

## Gestures

<iframe width="560" height="315" src="https://www.youtube.com/embed/dahwnujrd2k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

New diffusion model for gestures (WIP):

<iframe width="560" height="315" src="https://www.youtube.com/embed/gpyAwC1bQtY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</br>

## Robotics

<iframe width="560" height="315" src="https://www.youtube.com/embed/M7TdOx9WANM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</br>


All these demos are based on the [Transflower](/transflower) architecure. I am now working on a model based on diffusion that should allow for faster training and inference.
